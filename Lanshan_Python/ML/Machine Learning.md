
# 1、逻辑回归（Logistic Regression）


[Logistic（逻辑斯蒂）函数浅谈]：

https://zhuanlan.zhihu.com/p/630739668

当你的目标变量是分类变量时，才会考虑逻辑回归，并且主要用于两分类问题。

在[模式识别](#一什么是模式识别)问题中，所关心的量是分类，比如是否会患有某种疾病，这时就不能用简单的线性回归来完成这个问题了。为了解决次问题，我们引入了非线性激活函数g : $g:R^D\rightarrow(0,1)$
 →(0,1)来预测类别标签的[后验概率](#后验概率)p ( y = 1 ∣ x ) p(y=1|\bf x)p(y=1∣x)，其中y ∈ { 0 , 1 } y\in\{0,1\}y∈{0,1}，函数g gg的作用是把线性函数的值域从实数区间挤压到0和1之间
在Logistic回归中，激活函数的表达式为：

逻辑回归以线性回归为理论支持，又通过[Sigmoid函数](#sigmoid-函数)引入了非线性因素，因此可以轻松处理0/1分类问题。

逻辑回归的核心问题
逻辑回归的核心问题就是要找到这样一个界线，它尽可能将两类数据进行划分。这个界的方程为z = XW = w0+w1 x1+w2 x2 （此方程针对于2-features的情况，3-features在其后+w3x3即可），稍微强调一点，决策边界这一条线上的概率均为0.5。

那么，我们的核心问题为：求解“决策边界”的方程，即求解w0, w1, w2。并使其达到图2所示效果：
Logistic 激活函数又叫作 [Sigmoid 函数](#sigmoid-函数)，它将实数值压缩进 0 到 1 的区间内，还可以在预测概率的输出层中使用。该函数将大的负数转换为趋近于 0，将大的正数转换为趋近于 1。


                        


Logistic 分布是由其位置和尺度参数定义的连续分布。Logistic 分布的形状与正态分布的形状相似，但是 Logistic 分布的尾部更长，所以我们可以使用 Logistic 分布来建模比正态分布具有更长尾部和更高波峰的数据分布。
在深度学习中常用到的 Sigmoid 函数就是 Logistic 的分布函数在 
 的特殊形式。




###### 一、什么是模式识别
人类每时每刻在完成某种模式识别的任务，

例如读书看报（文字识别）、茫茫人海中寻找一个伙伴（特征识别）、鸟鸣（声音识别)……人们对外界对象的几乎所有认识都是对类别的认识，

通过计算机模仿人脑对现实世界各种事物 进行描述、分类、判断和识别的过程即为模式识别。那么什么是模式呢？

顾名思义，模式即为模式识别的对象，可以是一些文字，一个人，一张纸，鸟语花香等等一切可以进行识别测量的生活中的事物。

##### 后验概率
后验概率是指在得到“结果”的信息后重新修正的概率，是贝叶斯定理的核心概念之一。在统计学中，后验概率通常用于描述在观察到某些数据后，某个假设或模型成立的概率。

具体来说，假设我们有一个假设H和一个观测结果E。在观测到E之前，我们对H的信任度可以用先验概率P(H)来表示。然而，当我们观测到E后，我们对H的信任度会发生变化，这个新的信任度就是后验概率P(H|E)。

根据贝叶斯定理，后验概率P(H|E)可以通过以下公式计算：

P(H|E) = (P(E|H) × P(H)) / P(E)

其中：

P(H|E) 是后验概率，即在观测到E后H成立的概率。
P(E|H) 是似然函数，表示在假设H成立的情况下观测到E的概率。
P(H) 是先验概率，即在观测到任何数据之前对H的信任度。
P(E) 是观测到E的总概率，可以通过对所有可能的假设H’求和 P(E|H’) × P(H’) 来计算。


##### Sigmoid 函数
**优点：**

Sigmoid函数的优点在于它可导，并且值域在0到1之间，使得神经元的输出标准化。

**缺点：**

（1）梯度消失：Sigmoid 函数值在趋近 0 和 1 的时候函数值会变得平坦，梯度趋近于 0。

（2）不以零为中心：sigmoid函数的输出恒为正值，不是以零为中心的，这会导致权值更新时只能朝一个方向更新，从而影响收敛速度。

（3）计算成本高昂：exp() 函数与其他非线性激活函数相比，计算成本高昂。

（4）梯度爆炸：x值在趋近0的左右两边时，会造成梯度爆炸情况


# 支持向量机
支持向量机（Support Vector Machine，简称SVM）是一种监督学习算法，广泛应用于分类和回归任务。

SVM学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。如下图所示，

即为分离超平面，对于线性可分的数据集来说，这样的超平面有无穷多个（即[感知机]），但是几何间隔最大的分离超平面却是唯一的

SVM的核心思想是寻找一个超平面，使得两个类别之间的间隔最大化，从而实现良好的分类效果。

由简至繁的模型包括：

- 当训练样本**线性可分**时，通过**硬间隔最大化**，学习一个**线性可分支持向量机**
- 当训练样本**近似线性可分**时，通过**软间隔最大化**，学习一个**线性支持向量机**
- 当训练样本**线性不可分**时，通过**核技巧和软间隔最大化**，学习一个**非线性支持向量机**

##### 1、最大间隔超平面

在说明最大间隔超平面问题之前，先说明一下什么是线性可分。

[线性可分]：假设有猪狗两类样本，若在特征空间中存在一个超平面将猪狗完全准确分开，那么就称这个猪狗样本集线性可分。在二维空间时，超平面就是一条直线。


对于一个线性可分样本集，可能存在多个超平面（line1，line2）都可以将其线性可分，那么怎么选择超平面呢？

假设现在有一只冒充猪的小狗来搞破坏，是否存在一个超平面，无论来了多少只小狗或者小猪，依然可以将其阴谋识破？

SVM解决的就是这个，找出一个最佳超平面正确划分样本。

那最佳超平面长什么样的呢？我们认为最佳超平面必须具有更好的泛化能力，对噪声更为不敏感，即更好的[鲁棒性](#鲁棒性)。

从几何角度来说，两样本到超平面的间隔越大，抗干扰能力越强。

所以最佳超平面就是以最大间隔把样本分开的超平面，也称之为[最大间隔超平面]。

间隔是两侧样本到超平面的距离之和，即margin = d1+d2，多个样本就有多个间隔值，那是不是每个间隔对超平面的贡献都一样的呢？

答案是否定的，离超平面越近的样本越容易划分错误，因此离超平面越近的样本对超平面的影响越大，


所以为了找到最大间隔超平面，首先要找到两侧离超平面最近的样本点，求出其到超平面的距离之和，即margin = min（d1+d2）。

然后不同超平面，margin不同，为了找到最佳超平面，我们需要最大化margin，可以理解为泛化能力最大的那个超平面，即max margin。

##### 鲁棒性
是指系统或者器件在不同的环境或者条件下，能够保持其功能或者性能的特性

##### 核函数
当数据集在原始特征中不是线性可分的时候，支持向量机采用了引入映射函数的策略：

通过映射函数将原始特征空间映射为更高维的空间，在原始空间中不可分的数据在高维空间中可能变成可分，此时再在高维空间中运用SVM。


##### 惩罚因子：

惩罚因子用C表示，表示对错分样本的惩罚程度。

C越大对错分样本的惩罚就越大，趋向于对训练集全分对的情况，这样训练集测试时准确率很高，但是泛化能力弱，容易过拟合；

C值越小，对误分类的惩罚减小，允许容错，将他们视为噪声点，泛化能力较强。

凸函数
对区间 上定义的函数 f，若它对区间中任意两点 均有：

则称f为区间 上的凸函数，这和高数上讲图形的形状时是不同的概念。

形曲线的函数如就是凸函数。

对实数集上的函数，可通过求解二阶导数来判别：

若二阶导数在区间上非负，则称为凸函数
若二阶导数在区间上恒大于0，则称严格凸函数
仿射函数也是凸函数，只是不是严格凸函数。

凸优化问题
凸优化问题是特殊的约束最优化问题。其一般形式形式和约束最优化问题一样。

假设f、g、h在定义域内是连续可微的，且目标函数f和不等式约束函数g是凸函数，等式约束h是仿射函数（线性函数），则这种约束最优化问题称为凸优化问题。
因此凸优化问题特征的重要特征：

目标函数f，不等式约束函数g是凸函数
等式约束h是仿射函数
满足约束最优化问题的一般形式

凸二次规划问题的特征：

目标函数f是二次型函数函数
等式约束h是仿射函数
等式约g是仿射函数
满足约束最优化问题的一般形式
常用的二次规划问题求解方法有：

椭球法
内点法
增广拉格朗日法
梯度投影法

1. [逻辑回归](#1逻辑回归Logistic-Regression)
2、[朴素贝叶斯](#支持向量机)
3、感知机
4、支持向量机（SVM）
5、决策树
6、GBDT

7、XGBoost
8、LightGBM
9、PCA
10、K-means
11、EM算法
12、FM
13、梯度下降法
14、正则化
15、样本不均衡
16、特征工程
17、模型集成
18、kaggle
