
# 1、逻辑回归（Logistic Regression）


[Logistic（逻辑斯蒂）函数浅谈]：

https://zhuanlan.zhihu.com/p/630739668


在[模式识别](#一什么是模式识别)问题中，所关心的量是分类，比如是否会患有某种疾病，这时就不能用简单的线性回归来完成这个问题了。为了解决次问题，我们引入了非线性激活函数g : R D → ( 0 , 1 ) g:{\mathbb R}^D\to(0,1)g:R 
D
 →(0,1)来预测类别标签的[后验概率](#后验概率)p ( y = 1 ∣ x ) p(y=1|\bf x)p(y=1∣x)，其中y ∈ { 0 , 1 } y\in\{0,1\}y∈{0,1}，函数g gg的作用是把线性函数的值域从实数区间挤压到0和1之间
在Logistic回归中，激活函数的表达式为：

逻辑回归以线性回归为理论支持，又通过[Sigmoid函数](#sigmoid-函数)引入了非线性因素，因此可以轻松处理0/1分类问题。

逻辑回归的核心问题
逻辑回归的核心问题就是要找到这样一个界线，它尽可能将两类数据进行划分。这个界的方程为z = XW = w0+w1 x1+w2 x2 （此方程针对于2-features的情况，3-features在其后+w3x3即可），稍微强调一点，决策边界这一条线上的概率均为0.5。

那么，我们的核心问题为：求解“决策边界”的方程，即求解w0, w1, w2。并使其达到图2所示效果：
Logistic 激活函数又叫作 [Sigmoid 函数](#sigmoid-函数)，它将实数值压缩进 0 到 1 的区间内，还可以在预测概率的输出层中使用。该函数将大的负数转换为趋近于 0，将大的正数转换为趋近于 1。


                        


Logistic 分布是由其位置和尺度参数定义的连续分布。Logistic 分布的形状与正态分布的形状相似，但是 Logistic 分布的尾部更长，所以我们可以使用 Logistic 分布来建模比正态分布具有更长尾部和更高波峰的数据分布。
在深度学习中常用到的 Sigmoid 函数就是 Logistic 的分布函数在 
 的特殊形式。


1. [逻辑回归](#1逻辑回归Logistic-Regression)
2、朴素贝叶斯
3、感知机
4、支持向量机（SVM）
5、决策树
6、GBDT

7、XGBoost
8、LightGBM
9、PCA
10、K-means
11、EM算法
12、FM
13、梯度下降法
14、正则化
15、样本不均衡
16、特征工程
17、模型集成
18、kaggle


###### 一、什么是模式识别
人类每时每刻在完成某种模式识别的任务，

例如读书看报（文字识别）、茫茫人海中寻找一个伙伴（特征识别）、鸟鸣（声音识别)……人们对外界对象的几乎所有认识都是对类别的认识，

通过计算机模仿人脑对现实世界各种事物 进行描述、分类、判断和识别的过程即为模式识别。那么什么是模式呢？

顾名思义，模式即为模式识别的对象，可以是一些文字，一个人，一张纸，鸟语花香等等一切可以进行识别测量的生活中的事物。

##### 后验概率
后验概率是指在得到“结果”的信息后重新修正的概率，是贝叶斯定理的核心概念之一。在统计学中，后验概率通常用于描述在观察到某些数据后，某个假设或模型成立的概率。

具体来说，假设我们有一个假设H和一个观测结果E。在观测到E之前，我们对H的信任度可以用先验概率P(H)来表示。然而，当我们观测到E后，我们对H的信任度会发生变化，这个新的信任度就是后验概率P(H|E)。

根据贝叶斯定理，后验概率P(H|E)可以通过以下公式计算：

P(H|E) = (P(E|H) × P(H)) / P(E)

其中：

P(H|E) 是后验概率，即在观测到E后H成立的概率。
P(E|H) 是似然函数，表示在假设H成立的情况下观测到E的概率。
P(H) 是先验概率，即在观测到任何数据之前对H的信任度。
P(E) 是观测到E的总概率，可以通过对所有可能的假设H’求和 P(E|H’) × P(H’) 来计算。


##### Sigmoid 函数
**优点：**

Sigmoid函数的优点在于它可导，并且值域在0到1之间，使得神经元的输出标准化。

**缺点：**

（1）梯度消失：Sigmoid 函数值在趋近 0 和 1 的时候函数值会变得平坦，梯度趋近于 0。

（2）不以零为中心：sigmoid函数的输出恒为正值，不是以零为中心的，这会导致权值更新时只能朝一个方向更新，从而影响收敛速度。

（3）计算成本高昂：exp() 函数与其他非线性激活函数相比，计算成本高昂。

（4）梯度爆炸：x值在趋近0的左右两边时，会造成梯度爆炸情况
